{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03bfc4ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Theo\\source\\repos\\AUEB\\LangChain\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1a99edb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# 1. SETUP (Φόρτωση πραγματικού μοντέλου)\n",
    "model_name = \"dslim/bert-base-NER\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b90df604",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. INPUT DATA\n",
    "text = \"Apple was founded by Steve Jobs in California.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc2517d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "tensor([[  101,  7302,  1108,  1771,  1118,  3036, 18235,  1116,  1107,  1756,\n",
      "           119,   102]])\n",
      "1. Input IDs Shape: torch.Size([1, 12])\n"
     ]
    }
   ],
   "source": [
    "# Tokenization\n",
    "# return_tensors=\"pt\": Φέρτο σε PyTorch Tensor\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "print(inputs['attention_mask'])\n",
    "print(inputs['input_ids'])\n",
    "print(f\"1. Input IDs Shape: {inputs['input_ids'].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe715a3",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'TokenClassifierOutput' object has no attribute 'start_logits'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m      3\u001b[39m     outputs = model(**inputs)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     \u001b[43moutputs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstart_logits\u001b[49m\n\u001b[32m      5\u001b[39m     logits = outputs.logits\n",
      "\u001b[31mAttributeError\u001b[39m: 'TokenClassifierOutput' object has no attribute 'start_logits'"
     ]
    }
   ],
   "source": [
    "# 3. INFERENCE\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3666998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TokenClassifierOutput(loss=None, logits=tensor([[[ 7.5362, -0.5904, -0.8138, -1.1949, -1.6195, -0.4572, -1.1535,\n",
      "          -1.4725, -1.1045],\n",
      "         [-0.5476,  0.0943, -1.3713,  1.2435, -2.4477,  7.8553, -1.7852,\n",
      "          -1.0442, -2.6816],\n",
      "         [ 9.9153, -1.2348, -0.8506, -2.0552, -2.3920, -1.3129,  0.3082,\n",
      "          -2.3134, -1.0793],\n",
      "         [ 9.8475, -0.9621, -1.3455, -1.6084, -2.6286, -0.7910,  0.1654,\n",
      "          -2.1704, -1.1955],\n",
      "         [10.6818, -1.3400, -1.3847, -1.1385, -2.1305, -1.5486, -0.6992,\n",
      "          -1.9663, -1.2993],\n",
      "         [-0.6558, -1.2828, -2.4333,  9.0901, -1.4148, -0.6516, -2.5449,\n",
      "          -0.6785, -1.6224],\n",
      "         [-1.2180, -1.3712, -1.2147, -1.7568,  8.8958, -1.8238,  0.7215,\n",
      "          -1.3092, -0.3991],\n",
      "         [ 0.6910, -2.1982, -1.4579, -0.9891,  6.6751, -2.5478,  0.6119,\n",
      "          -0.8784, -1.2875],\n",
      "         [10.5854, -1.3067, -0.9439, -1.7781, -2.2146, -1.7052, -0.2939,\n",
      "          -1.8484, -0.8532],\n",
      "         [-0.6665, -1.0404, -2.3527, -0.4044, -2.0379, -0.9513, -1.6543,\n",
      "           9.1457, -1.4401],\n",
      "         [10.5170, -1.3262, -0.9471, -1.7259, -2.1794, -1.6076, -0.4296,\n",
      "          -2.1951, -0.6599],\n",
      "         [ 5.4194,  0.0872, -1.5134, -0.3403, -1.2585, -1.2324, -0.8297,\n",
      "          -0.3929, -1.5550]]]), hidden_states=None, attentions=None)\n",
      "tensor([[[ 7.5362, -0.5904, -0.8138, -1.1949, -1.6195, -0.4572, -1.1535,\n",
      "          -1.4725, -1.1045],\n",
      "         [-0.5476,  0.0943, -1.3713,  1.2435, -2.4477,  7.8553, -1.7852,\n",
      "          -1.0442, -2.6816],\n",
      "         [ 9.9153, -1.2348, -0.8506, -2.0552, -2.3920, -1.3129,  0.3082,\n",
      "          -2.3134, -1.0793],\n",
      "         [ 9.8475, -0.9621, -1.3455, -1.6084, -2.6286, -0.7910,  0.1654,\n",
      "          -2.1704, -1.1955],\n",
      "         [10.6818, -1.3400, -1.3847, -1.1385, -2.1305, -1.5486, -0.6992,\n",
      "          -1.9663, -1.2993],\n",
      "         [-0.6558, -1.2828, -2.4333,  9.0901, -1.4148, -0.6516, -2.5449,\n",
      "          -0.6785, -1.6224],\n",
      "         [-1.2180, -1.3712, -1.2147, -1.7568,  8.8958, -1.8238,  0.7215,\n",
      "          -1.3092, -0.3991],\n",
      "         [ 0.6910, -2.1982, -1.4579, -0.9891,  6.6751, -2.5478,  0.6119,\n",
      "          -0.8784, -1.2875],\n",
      "         [10.5854, -1.3067, -0.9439, -1.7781, -2.2146, -1.7052, -0.2939,\n",
      "          -1.8484, -0.8532],\n",
      "         [-0.6665, -1.0404, -2.3527, -0.4044, -2.0379, -0.9513, -1.6543,\n",
      "           9.1457, -1.4401],\n",
      "         [10.5170, -1.3262, -0.9471, -1.7259, -2.1794, -1.6076, -0.4296,\n",
      "          -2.1951, -0.6599],\n",
      "         [ 5.4194,  0.0872, -1.5134, -0.3403, -1.2585, -1.2324, -0.8297,\n",
      "          -0.3929, -1.5550]]])\n"
     ]
    }
   ],
   "source": [
    "from torch import logit\n",
    "\n",
    "\n",
    "print(outputs)\n",
    "print(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716743d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2. Logits Shape: torch.Size([1, 12, 9])\n"
     ]
    }
   ],
   "source": [
    "print(f\"2. Logits Shape: {logits.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441770e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3. Predictions Shape: torch.Size([1, 12])\n",
      "Predictions: tensor([[0, 5, 0, 0, 0, 3, 4, 4, 0, 7, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "# 4. POST-PROCESSING (TENSOR MANIPULATION)\n",
    "\n",
    "# Βήμα Α: Βρες την κλάση με το μεγαλύτερο σκορ ΓΙΑ ΚΑΘΕ TOKEN.\n",
    "# dim=2: Σημαίνει \"σύγκρινε τους αριθμούς στον 3ο άξονα\n",
    "predictions = torch.argmax(logits, dim=2)\n",
    "\n",
    "print(f\"3. Predictions Shape: {predictions.shape}\")\n",
    "print(f\"Predictions: {predictions}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8730f16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs: [101, 7302, 1108, 1771, 1118, 3036, 18235, 1116, 1107, 1756, 119, 102]\n",
      "Prediction IDs: [0, 5, 0, 0, 0, 3, 4, 4, 0, 7, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "prediction_ids = predictions.squeeze().tolist()\n",
    "input_ids = inputs['input_ids'].squeeze().tolist()\n",
    "print(f\"Input IDs: {input_ids}\")\n",
    "print(f\"Prediction IDs: {prediction_ids}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dceb32f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- ΑΠΟΤΕΛΕΣΜΑΤΑ ---\n",
      "Token: Apple      | Label: B-ORG\n",
      "Token: Steve      | Label: B-PER\n",
      "Token: Job        | Label: I-PER\n",
      "Token: ##s        | Label: I-PER\n",
      "Token: California | Label: B-LOC\n"
     ]
    }
   ],
   "source": [
    "id2label = model.config.id2label\n",
    "\n",
    "\n",
    "print(\"\\n--- ΑΠΟΤΕΛΕΣΜΑΤΑ ---\")\n",
    "for i, token_id in enumerate(input_ids):\n",
    "    # Μετατροπή ID λέξης σε κείμενο\n",
    "    token_str = tokenizer.convert_ids_to_tokens(token_id)\n",
    "    \n",
    "    # Μετατροπή ID πρόβλεψης σε Label (π.χ. 'B-ORG')\n",
    "    label_id = prediction_ids[i]\n",
    "    label_str = id2label[label_id]\n",
    "    \n",
    "    # Φιλτράρισμα: Δεν μας νοιάζουν τα special tokens ([CLS], [SEP]) ούτε τα 'O' (Outside/Ασήμαντα)\n",
    "    if label_str != 'O' and token_str not in ['[CLS]', '[SEP]']:\n",
    "        print(f\"Token: {token_str:10} | Label: {label_str}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77e7d49",
   "metadata": {},
   "source": [
    "### word ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43dbbac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_bert = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "text = \"I love microservices architecture.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f6d649",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'love', 'microservices', 'architecture.']\n"
     ]
    }
   ],
   "source": [
    "words = text.split()\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a91add5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  1045,  2293, 12702,  8043,  7903,  2229,  4294,  1012,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "tensor([[  101,  1045,  2293, 12702,  8043,  7903,  2229,  4294,  1012,   102]])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "['[CLS]', 'i', 'love', 'micro', '##ser', '##vic', '##es', 'architecture', '.', '[SEP]']\n",
      "['[CLS]', 'i', 'love', 'micro', '##ser', '##vic', '##es', 'architecture', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "inputs_bert = tokenizer_bert(words, is_split_into_words=True, return_tensors=\"pt\")\n",
    "print(inputs_bert)\n",
    "print(inputs_bert['input_ids'])\n",
    "print(inputs_bert['attention_mask'])\n",
    "tokens_bert = tokenizer_bert.convert_ids_to_tokens(inputs_bert['input_ids'].squeeze().tolist())\n",
    "tokens_bert_without_squeeze = tokenizer_bert.convert_ids_to_tokens(inputs_bert['input_ids'].tolist()[0])\n",
    "print(tokens_bert_without_squeeze)\n",
    "print(tokens_bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f50a4cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, 0, 1, 2, 2, 2, 2, 3, 3, None]\n",
      "[None, 0, 1, 2, 2, 2, 2, 3, 3, None]\n"
     ]
    }
   ],
   "source": [
    "word_ids_bert = inputs_bert.word_ids()\n",
    "print(word_ids_bert)\n",
    "print(inputs_bert.word_ids(batch_index=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7a204b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    outputs_bert = model(**inputs_bert)\n",
    "    logits_bert = outputs_bert.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240b7846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions BERT Shape: torch.Size([1, 10])\n",
      "Predictions BERT: tensor([[0, 3, 3, 4, 4, 0, 0, 0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "predictions_bert = torch.argmax(logits_bert, dim=2)\n",
    "print(f\"Predictions BERT Shape: {predictions_bert.shape}\")\n",
    "print(f\"Predictions BERT: {predictions_bert}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75fc1ff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- BERT TOKENIZATION ΑΠΟΤΕΛΕΣΜΑΤΑ ---\n",
      "Token: I               | Label: B-PER\n",
      "Token: love            | Label: B-PER\n",
      "Token: microservices   | Label: I-PER\n",
      "Token: microservices   | Label: I-PER\n"
     ]
    }
   ],
   "source": [
    "aligned_labels = []\n",
    "for i, word_id in enumerate(word_ids_bert):\n",
    "    if word_id is None:\n",
    "        continue\n",
    "    label_id = predictions_bert[0][i].item()\n",
    "    label_str = id2label[label_id]\n",
    "    aligned_labels.append((words[word_id], label_str))\n",
    "\n",
    "print(\"\\n--- BERT TOKENIZATION ΑΠΟΤΕΛΕΣΜΑΤΑ ---\")\n",
    "for token, label in aligned_labels:\n",
    "    if label != 'O':\n",
    "        print(f\"Token: {token:15} | Label: {label}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
