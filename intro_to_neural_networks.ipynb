{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de027d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "torch.manual_seed(0)\n",
    "from custom_torchinfo import custom_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a5f4ea19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "============================================================================================================================================\n",
      "Layer (type:depth-idx)                   Input Shape               Output Shape              Param #                   Trainable\n",
      "============================================================================================================================================\n",
      "SimpleMLP                                [64, 128]                 [64, 1]                   --                        True\n",
      "├─Linear: 1-1                            [64, 128]                 [64, 516]                 66,564                    True\n",
      "├─ReLU: 1-2                              [64, 516]                 [64, 516]                 --                        --\n",
      "├─Linear: 1-3                            [64, 516]                 [64, 516]                 266,772                   True\n",
      "├─ReLU: 1-4                              [64, 516]                 [64, 516]                 --                        --\n",
      "├─Linear: 1-5                            [64, 516]                 [64, 1]                   517                       True\n",
      "============================================================================================================================================\n",
      "Total params: 333,853\n",
      "Trainable params: 333,853\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (Units.MEGABYTES): 21.37\n",
      "============================================================================================================================================\n",
      "Input size (MB): 0.03\n",
      "Forward/backward pass size (MB): 0.53\n",
      "Params size (MB): 1.34\n",
      "Estimated Total Size (MB): 1.90\n",
      "============================================================================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "============================================================================================================================================\n",
       "Layer (type:depth-idx)                   Input Shape               Output Shape              Param #                   Trainable\n",
       "============================================================================================================================================\n",
       "SimpleMLP                                [64, 128]                 [64, 1]                   --                        True\n",
       "├─Linear: 1-1                            [64, 128]                 [64, 516]                 66,564                    True\n",
       "├─ReLU: 1-2                              [64, 516]                 [64, 516]                 --                        --\n",
       "├─Linear: 1-3                            [64, 516]                 [64, 516]                 266,772                   True\n",
       "├─ReLU: 1-4                              [64, 516]                 [64, 516]                 --                        --\n",
       "├─Linear: 1-5                            [64, 516]                 [64, 1]                   517                       True\n",
       "============================================================================================================================================\n",
       "Total params: 333,853\n",
       "Trainable params: 333,853\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 21.37\n",
       "============================================================================================================================================\n",
       "Input size (MB): 0.03\n",
       "Forward/backward pass size (MB): 0.53\n",
       "Params size (MB): 1.34\n",
       "Estimated Total Size (MB): 1.90\n",
       "============================================================================================================================================"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class SimpleMLP(nn.Module):\n",
    "    def __init__(self, input_size=128, hidden_size=516, output_size=1):\n",
    "        super().__init__()\n",
    "        self.fc1   = nn.Linear(input_size, hidden_size)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2   = nn.Linear(hidden_size, hidden_size)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3   = nn.Linear(hidden_size, output_size) \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu1(self.fc1(x))\n",
    "        x = self.relu2(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "model = SimpleMLP().to(device)\n",
    "\n",
    "custom_summary(model, input_size=(64, 128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29051fc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Model Size] 1,335,412 bytes ~ 1.274 MB\n"
     ]
    }
   ],
   "source": [
    "def model_size_bytes(model):\n",
    "    \"\"\"Return model size in bytes (params + buffers).\"\"\"\n",
    "    model_params = list(model.parameters()) + list(model.buffers())\n",
    "    size = 0\n",
    "    for t in model_params:\n",
    "        size += t.numel() * t.element_size()\n",
    "    return size\n",
    "\n",
    "size_bytes = model_size_bytes(model)\n",
    "\n",
    "# Show output\n",
    "print(f\"[Model Size] {size_bytes:,} bytes ~ {size_bytes / (1024**2):.3f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a0c7a256",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test batches\n",
    "\n",
    "x_base = torch.randn(1, 128, device=device)\n",
    "batch_size1 = 64\n",
    "x1 = x_base.expand(batch_size1, -1).contiguous()\n",
    "batch_size2 = 8\n",
    "x2 = x_base.expand(batch_size2, -1).contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "db5e8259",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Latency] 0.342 ms per forward pass (batch size=64)\n",
      "[Latency] 0.312 ms per forward pass (batch size=8)\n"
     ]
    }
   ],
   "source": [
    "def measure_latency(model, x, iters=50):\n",
    "    \"\"\"Return average latency (ms) per forward pass.\"\"\"\n",
    "    model.eval()\n",
    "    start = time.perf_counter()\n",
    "    with torch.inference_mode():\n",
    "        for _ in range(iters):\n",
    "            _ = model(x)\n",
    "            if x.device.type == \"cuda\":\n",
    "                torch.cuda.synchronize()\n",
    "    elapsed = time.perf_counter() - start\n",
    "    return (elapsed / iters) * 1000  # Convert seconds to milliseconds\n",
    "\n",
    "latency_x1 = measure_latency(model, x1) \n",
    "latency_x2 = measure_latency(model, x2)\n",
    "\n",
    "print(f\"[Latency] {latency_x1:.3f} ms per forward pass (batch size={batch_size1})\")\n",
    "print(f\"[Latency] {latency_x2:.3f} ms per forward pass (batch size={batch_size2})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c76d707",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GPU Memory] Current allocated: 10.44 MB | Peak during forward: 10.81 MB (batch size=64)\n",
      "[GPU Memory] Current allocated: 10.44 MB | Peak during forward: 10.48 MB (batch size=8)\n"
     ]
    }
   ],
   "source": [
    "def measure_gpu_memory(model, x):\n",
    "    \"\"\"Return current and peak GPU memory in MB after one forward.\"\"\"\n",
    "    if not torch.cuda.is_available():\n",
    "        return (0.0, 0.0)  # Return zeros if CUDA not available\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    try:\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "    except AttributeError:\n",
    "        pass\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        y = model(x)\n",
    "        \n",
    "    torch.cuda.synchronize()\n",
    "    current = torch.cuda.memory_allocated() / (1024**2)\n",
    "    peak    = torch.cuda.max_memory_allocated() / (1024**2)\n",
    "    return current, peak\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    mem1 = measure_gpu_memory(model, x1)\n",
    "    mem2 = measure_gpu_memory(model, x2)\n",
    "    print(f\"[GPU Memory] Current allocated: {mem1[0]:.2f} MB | Peak during forward: {mem1[1]:.2f} MB (batch size={batch_size1})\")\n",
    "    print(f\"[GPU Memory] Current allocated: {mem2[0]:.2f} MB | Peak during forward: {mem2[1]:.2f} MB (batch size={batch_size2})\")\n",
    "else:\n",
    "    print(\"[GPU Memory] CUDA not available - running on CPU (no GPU memory tracking)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "afab7bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc, torch\n",
    "\n",
    "del model, x1, x2 \n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    torch.cuda.synchronize()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
